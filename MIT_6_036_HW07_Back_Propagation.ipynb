{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT 6.036 HW07 Back Propagation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "y1EffzDFkqMX"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TangJiahui/6.036_Machine_Learning/blob/main/MIT_6_036_HW07_Back_Propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A"
      },
      "source": [
        "#MIT 6.036 Fall 2020: Homework 7 Neural Network - Back Propagation#\n",
        "\n",
        "This colab notebook provides code and a framework for problem 2 of [the homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "**Note**: You can go to `File > Save a copy in Drive...` to save your own copy of this notebook for editing.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YM-_zLf9Bp-",
        "outputId": "dbeb6f29-30a7-442b-d868-6d06665643af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!rm -rf code_for_hw7*\n",
        "!rm -rf mnist\n",
        "!rm -rf data\n",
        "!wget --no-check-certificate --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw07/code_for_hw7.zip\n",
        "!unzip code_for_hw7.zip\n",
        "!mv code_for_hw7/* .\n",
        "\n",
        "from code_for_hw7 import *\n",
        "import numpy as np\n",
        "import modules_disp as disp\n"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_hw7.zip\n",
            "   creating: code_for_hw7/\n",
            "  inflating: code_for_hw7/code_for_hw7.py  \n",
            "  inflating: code_for_hw7/pytorch_code_for_hw7.py  \n",
            "  inflating: code_for_hw7/code_for_hw7_pytorch.py  \n",
            "   creating: code_for_hw7/data/\n",
            "  inflating: code_for_hw7/data/data5_validate.csv  \n",
            "  inflating: code_for_hw7/data/data5_train.csv  \n",
            "  inflating: code_for_hw7/data/data1_train.csv  \n",
            "  inflating: code_for_hw7/data/data1_validate.csv  \n",
            "  inflating: code_for_hw7/data/data2_train.csv  \n",
            "  inflating: code_for_hw7/data/data2_validate.csv  \n",
            "  inflating: code_for_hw7/data/data3_train.csv  \n",
            "  inflating: code_for_hw7/data/data3_validate.csv  \n",
            "  inflating: code_for_hw7/data/data3class_train.csv  \n",
            "  inflating: code_for_hw7/data/data4_train.csv  \n",
            "  inflating: code_for_hw7/data/data4_validate.csv  \n",
            "  inflating: code_for_hw7/data/dataXor_train.csv  \n",
            "  inflating: code_for_hw7/expected_results.py  \n",
            "  inflating: code_for_hw7/modules_disp.py  \n",
            "  inflating: code_for_hw7/utils_hw7.py  \n",
            "   creating: code_for_hw7/__pycache__/\n",
            "  inflating: code_for_hw7/__pycache__/modules_disp.cpython-38.pyc  \n",
            "  inflating: code_for_hw7/__pycache__/expected_results.cpython-38.pyc  \n",
            "mv: cannot move 'code_for_hw7/__pycache__' to './__pycache__': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFxhrJ5XDlvb"
      },
      "source": [
        "# 2) Implementing Neural Networks\n",
        "\n",
        "This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
        " \n",
        "<br>\n",
        " \n",
        "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n",
        "\n",
        "specified by a weight matrix $W$ and a bias vector $W_0$. The input is $[x_1, \\ldots, x_m]^T$. The output is $[z_1, \\ldots, z_n]^T$\n",
        "\n",
        "<br>\n",
        "\n",
        "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjQgtwPHj08n"
      },
      "source": [
        "Although for \"real\" applications you want to use one of the many packaged implementations of neural networks (we'll start using one of those soon), there is no substitute for implementing one yourself to get an in-depth understanding. Luckily, that is relatively easy to do if we're not too concerned with maximum efficiency.\n",
        "\n",
        "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
        "\n",
        "```\n",
        "# build a 3-layer network\n",
        "net = Sequential([Linear(2,3), Tanh(),\n",
        "                  Linear(3,3), Tanh(),\n",
        "    \t          Linear(3,2), SoftMax()])\n",
        "# train the network on data and labels\n",
        "net.sgd(X, Y)\n",
        "```\n",
        "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwpgsbnho9K"
      },
      "source": [
        "## Linear Modules: ##\n",
        "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n",
        "\n",
        "Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights.\n",
        "\n",
        "Hint: be careful with dimensions when computing dLdW0. dLdZ is (n x b), but dLdW0 is (n x 1). Why do you need to sum over all $b$ points in the batch when computing dLdW0?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VsYLAxCfy7U"
      },
      "source": [
        "# class Linear(Module):\n",
        "#     def __init__(self, m, n):\n",
        "#         self.m, self.n = (m, n)  # (in size, out size)\n",
        "#         self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
        "#         self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
        "\n",
        "#     def forward(self, A):\n",
        "#         self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
        "#         #return self.W.T@A+self.W0*np.ones([1, self.A.shape[1]])  # Your code (n x b)\n",
        "#         return np.dot(self.W.T, A) + self.W0\n",
        "\n",
        "#     def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
        "#         self.dLdW = self.A@dLdZ.T       \n",
        "#         #self.dLdW0 = dLdZ@np.ones([self.A.shape[1],1])    \n",
        "#         self.dLdW0 = dLdZ.sum(axis=1).reshape((self.n, 1))  # (n x 1)\n",
        "#         return self.W@dLdZ     #return dLdA (m x b)\n",
        "\n",
        "#     def sgd_step(self, lrate):  # Gradient descent step\n",
        "#         self.W = self.W -  lrate*self.dLdW\n",
        "#         self.W0 = self.W0 - lrate*self.dLdW0"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3_GZGQo_9_b"
      },
      "source": [
        "class Module:\n",
        "    def sgd_step(self, lrate): pass  # For modules w/o weights\n",
        "\n",
        "class Linear(Module):\n",
        "    def __init__(self, m, n):\n",
        "        self.m, self.n = (m, n)  # (in size, out size)\n",
        "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
        "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.dot(self.W.T, A) + self.W0  # (m x n)^T (m x b) = (n x b)\n",
        "\n",
        "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
        "        self.dLdW = np.dot(self.A, dLdZ.T)                  # (m x n)\n",
        "        self.dLdW0 = dLdZ.sum(axis=1).reshape((self.n, 1))  # (n x 1)\n",
        "        return np.dot(self.W, dLdZ)                         # (m x b)\n",
        "\n",
        "    def sgd_step(self, lrate):  # Gradient descent step\n",
        "        self.W -= lrate*self.dLdW\n",
        "        self.W0 -= lrate*self.dLdW0"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqZ7_kZYr5s5"
      },
      "source": [
        " You are **highly encouraged** to make your own tests for each module. The test cases being run on catsoop are given below for your reference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY3yePY0r4eA",
        "outputId": "9b773954-68fb-466b-c408-224d2b601a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def nn_linear_forward():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    X,Y = super_simple_separable()\n",
        "    return linear.forward(X).tolist()\n",
        "\n",
        "expected_linear_forward = [[10.417500637754383, 6.911221682745654, 20.733665048236965, 22.891234399772113], [7.168722346625092, 3.489987464919749, 10.469962394759248, 9.998261102396512], [-2.071054548689073, 0.6941371647696142, 2.0824114943088414, 4.849668106971125]]\n",
        "print(\"Pass linear forward? \", np.allclose(nn_linear_forward(), expected_linear_forward))\n",
        "\n",
        "def nn_linear_forward_bias():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    linear.W0 = np.array([[1],[1],[1]])\n",
        "    X,Y = super_simple_separable()\n",
        "    return linear.forward(X).tolist()\n",
        "\n",
        "expected_linear_forward_bias = [[11.417500637754383, 7.911221682745654, 21.733665048236965, 23.891234399772113], [8.168722346625092, 4.489987464919749, 11.469962394759248, 10.998261102396512], [-1.071054548689073, 1.6941371647696142, 3.0824114943088414, 5.849668106971125]]\n",
        "print(\"Pass linear forward bias? \", np.allclose(nn_linear_forward_bias(), expected_linear_forward_bias))\n"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass linear forward?  True\n",
            "Pass linear forward bias?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2oTBRYETrI_",
        "outputId": "e29ae425-fae6-4653-9542-7bde31ee8a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def nn_linear_backward():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    X,Y = super_simple_separable()\n",
        "    linear.forward(X)\n",
        "    dLdZ = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0],\n",
        "                                     [3, 0, 0, 1]])\n",
        "    return linear.backward(dLdZ).tolist()\n",
        "\n",
        "expected_linear_backward = [[3.889497924054116, 1.247373376201773, 0.2829538755771419, 0.6920722655660196], [2.1525571673658237, 1.5845507770701677, 1.3205629190941617, -0.6910398159642225]]\n",
        "print(\"Pass linear backward? \", np.allclose(nn_linear_backward(), expected_linear_backward))\n",
        "\n",
        "def nn_linear_backward_stored_dLdW_dLdW0():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    X,Y = super_simple_separable()\n",
        "    linear.forward(X)\n",
        "    dLdZ = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0],\n",
        "                                     [3, 0, 0, 1]])\n",
        "    linear.backward(dLdZ)\n",
        "    return [linear.dLdW.tolist(), linear.dLdW0.tolist()]\n",
        "\n",
        "expected_linear_backward_stored = [[[5, 13, 18], [7, 16, 20]], [[2], [3], [4]]]\n",
        "print(\"Pass linear backward stored vals dLdW? \", np.allclose(nn_linear_backward_stored_dLdW_dLdW0()[0], expected_linear_backward_stored[0]))\n",
        "print(\"Pass linear backward stored vals dLdW0? \", np.allclose(nn_linear_backward_stored_dLdW_dLdW0()[1], expected_linear_backward_stored[1]))\n",
        "\n",
        "def nn_linear_sgd():\n",
        "    np.random.seed(0)\n",
        "    linear = Linear(2,3)\n",
        "    X,Y = super_simple_separable()\n",
        "    linear.forward(X)\n",
        "    dLdZ = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0],\n",
        "                                     [3, 0, 0, 1]])\n",
        "    linear.backward(dLdZ)\n",
        "    linear.sgd_step(0.005)\n",
        "    return [np.vstack([linear.W, linear.W0.T]).tolist()]\n",
        "\n",
        "expected_linear_sgd = [[[1.222373376201773, 0.2179538755771419, 0.6020722655660197], [1.5495507770701678, 1.2405629190941616, -0.7910398159642225], [-0.01, -0.015, -0.02]]]\n",
        "print(\"Pass linear sgd? \", np.allclose(nn_linear_sgd(), expected_linear_sgd))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass linear backward?  True\n",
            "Pass linear backward stored vals dLdW?  True\n",
            "Pass linear backward stored vals dLdW0?  True\n",
            "Pass linear sgd?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ETL01mPsBz4"
      },
      "source": [
        "The following datasets are defined for your use:\n",
        "*  `super_simple_separable_through_origin()`\n",
        "*  `super_simple_separable()`\n",
        "*  `xor()`\n",
        "*  `xor_more()`\n",
        "*  `hard()`\n",
        "\n",
        "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
        "```\n",
        "def plot_nn(X, Y, nn):\n",
        "    \"\"\" Plot output of nn vs. data \"\"\"\n",
        "    def predict(x):\n",
        "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
        "    xmin, ymin = np.min(X, axis=1)-1\n",
        "    xmax, ymax = np.max(X, axis=1)+1\n",
        "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
        "    plot_data(X, Y, nax)\n",
        "    plt.show()```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s70beWJh09h"
      },
      "source": [
        "## Activation functions: ##\n",
        "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
        "\n",
        "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwaNAtLnhenT"
      },
      "source": [
        "### Tanh: ###\n",
        "Please use np.tanh here. You can find documentation online.\n",
        "\n",
        "Hint: the derivative of $\\tanh$ is given by $\\frac{d\\tanh(z)}{d z} = 1 - \\tanh(z)^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6eD3dnftiR"
      },
      "source": [
        "class Tanh(Module):            # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.tanh(Z)    \n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # Uses stored self.A\n",
        "        return dLdA * (1 - np.tanh(np.arctanh(self.A))**2)  # Your code: return dLdZ with dimensions (?, b)"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpo88_6wQh0R"
      },
      "source": [
        "Tanh unit test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZV85HbJQlhc",
        "outputId": "5304e49e-c9bc-4de2-871c-b29a02d0e20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def nn_tanh_forward():\n",
        "    np.random.seed(0)\n",
        "    tanh = Tanh()\n",
        "    X,Y = super_simple_separable()\n",
        "    return tanh.forward(X).tolist()\n",
        "\n",
        "expected_tanh_forward = [[0.9640275800758169, 0.9950547536867305, 0.999999969540041, 0.9999999999244973], [0.9999092042625951, 0.9640275800758169, 0.9999877116507956, 0.9999092042625951]]\n",
        "print(\"Pass tanh forward? \", np.allclose(nn_tanh_forward(), expected_tanh_forward))\n",
        "\n",
        "def nn_tanh_backward():\n",
        "    np.random.seed(0)\n",
        "    tanh = Tanh()\n",
        "    X,Y = super_simple_separable()\n",
        "    tanh.forward(X)\n",
        "    dLdA = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0]])\n",
        "    return tanh.backward(dLdA).tolist()\n",
        "\n",
        "expected_tanh_backward = [[0.07065082485316443, 0.009866037165440211, 0.0, 0.0], [0.0003631664618877206, 0.0, 2.4576547405286142e-05, 0.0]]\n",
        "print(\"Pass tanh backward? \", np.allclose(nn_tanh_backward(), expected_tanh_backward))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass tanh forward?  True\n",
            "Pass tanh backward?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FW7ocKRhcgY"
      },
      "source": [
        "### ReLU: ###\n",
        "Hint:\n",
        "[`np.maximum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html) might be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fm2KsLUfqdp"
      },
      "source": [
        "class ReLU(Module): # Layer activation\n",
        "    def forward(self, Z): \n",
        "        self.A = np.maximum(0,Z)  \n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA): # uses stored self.A\n",
        "        if np.sum(self.A) == 0:\n",
        "            return dLdA*np.zeros([1,self.A.shape[1]]) \n",
        "        else:\n",
        "            return dLdA*np.ones([1,self.A.shape[1]])           # Your code: return dLdZ (?, b)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMxtp-BJRyus"
      },
      "source": [
        "ReLU unit tests:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9NDufA7R16i",
        "outputId": "5f3c9e22-a423-489d-9c5c-87bfb8ec9f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def nn_relu_forward():\n",
        "    np.random.seed(0)\n",
        "    relu = ReLU()\n",
        "    X,Y = super_simple_separable()\n",
        "    return relu.forward(X).tolist()\n",
        "\n",
        "expected_relu_forward = [[2, 3, 9, 12], [5, 2, 6, 5]]\n",
        "print(\"Pass relu forward? \", np.allclose(nn_relu_forward(), expected_relu_forward))\n",
        "\n",
        "def nn_relu_backward():\n",
        "    np.random.seed(0)\n",
        "    relu = ReLU()\n",
        "    X,Y = super_simple_separable()\n",
        "    relu.forward(X)\n",
        "    dLdA = np.array([[1, 1, 0, 0],\n",
        "                                     [2, 0, 1, 0]])\n",
        "    return relu.backward(dLdA).tolist()\n",
        "\n",
        "expected_relu_backward = [[1, 1, 0, 0], [2, 0, 1, 0]]\n",
        "print(\"Pass relu backward? \", np.allclose(nn_relu_backward(), expected_relu_backward))\n"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass relu forward?  True\n",
            "Pass relu backward?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKtXuTQ0hSNO"
      },
      "source": [
        "###SoftMax: ###\n",
        "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point. The output should be a 1D numpy array. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqK-CJrnfn22"
      },
      "source": [
        "class SoftMax(Module):# Output activation\n",
        "    def forward(self, Z): #z= [n*b]\n",
        "        return (np.exp(Z)/np.sum(np.exp(Z), axis = 0))# Your code: (?, b)\n",
        "\n",
        "    def backward(self, dLdZ):# Assume that dLdZ is passed in\n",
        "        return dLdZ\n",
        "\n",
        "    def class_fun(self, Ypred): \n",
        "        return np.argmax(Ypred, axis=0) # Your code: A 1D vector (b, ) "
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DrQDUepW5gd"
      },
      "source": [
        "Test cases for softmax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-7HrY_4W7yW",
        "outputId": "8cd8f163-d831-4bd9-fc8e-d697221cfae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def nn_softmax_forward():\n",
        "    np.random.seed(0)\n",
        "    softmax = SoftMax()\n",
        "    X,Y = super_simple_separable()\n",
        "    return softmax.forward(X).tolist()\n",
        "\n",
        "expected_softmax_forward = [[0.04742587317756679, 0.7310585786300048, 0.9525741268224334, 0.9990889488055993], [0.9525741268224333, 0.2689414213699951, 0.04742587317756678, 0.0009110511944006454]]\n",
        "print(\"Pass softmax forward? \", np.allclose(nn_softmax_forward(), expected_softmax_forward))\n",
        "  \n",
        "def nn_softmax_class_fun():\n",
        "    np.random.seed(0)\n",
        "    softmax = SoftMax()\n",
        "    X,Y = super_simple_separable()\n",
        "    Ypred = softmax.forward(X)\n",
        "    return softmax.class_fun(Ypred).tolist()\n",
        "\n",
        "expected_softmax_class_fun = [1, 0, 0, 0]\n",
        "print(\"Pass softmax class fun? \", np.allclose(nn_softmax_class_fun(), expected_softmax_class_fun))"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass softmax forward?  True\n",
            "Pass softmax class fun?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZc7HnMSh4fn"
      },
      "source": [
        "## Loss Functions:##\n",
        "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value. Each column of Y will correspond to a one-hot vector encoding the correct class label for one point in our batch. \n",
        "\n",
        "The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4uy0pHVhNd8"
      },
      "source": [
        "### NLL: ###\n",
        "You should use multi-class NLL. \n",
        "\n",
        "Hint: $$\\frac{dNLL(Softmax(z))}{dz} = Y_{pred} - Y$$\n",
        "\n",
        "As an exercise, try proving that this is true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Fb8mimflgb"
      },
      "source": [
        "class NLL(Module):       # Loss\n",
        "    def forward(self, Ypred, Y):\n",
        "        self.Ypred = Ypred\n",
        "        self.Y = Y\n",
        "        return -np.sum(Y*np.log(Ypred))      # Your code\n",
        "\n",
        "    def backward(self):  # Use stored self.Ypred, self.Y\n",
        "        return self.Ypred - self.Y      # Your code"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhL4QL5xYqYe"
      },
      "source": [
        "NLL Test Cases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB6Y0Tl5Ys7v",
        "outputId": "ebd7c320-0d1d-4621-b140-4c07f87069b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def nn_nll_forward():\n",
        "    nll = NLL()\n",
        "    y = np.array([[1, 0, 1, 0]])\n",
        "    Y = for_softmax(y)\n",
        "    ypred = np.array([[.7, .3, .99, .99]])\n",
        "    Ypred = for_softmax(ypred)\n",
        "    return nll.forward(Ypred, Y)\n",
        "\n",
        "expected_nll_forward = 5.328570409719057\n",
        "print(\"Pass nll forward? \", np.allclose(nn_nll_forward(), expected_nll_forward))\n",
        "\n",
        "def nn_nll_backward():\n",
        "    nll = NLL()\n",
        "    y = np.array([[1, 0, 1, 0]])\n",
        "    Y = for_softmax(y)\n",
        "    ypred = np.array([[.7, .3, .99, .99]])\n",
        "    Ypred = for_softmax(ypred)\n",
        "    nll.forward(Ypred, Y)\n",
        "    return nll.backward().tolist()\n",
        "\n",
        "expected_nll_backward = [[0.30000000000000004, -0.30000000000000004, 0.010000000000000009, -0.99], [-0.30000000000000004, 0.3, -0.010000000000000009, 0.99]]\n",
        "print(\"Pass nll backward? \", np.allclose(nn_nll_backward(), expected_nll_backward))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pass nll forward?  True\n",
            "Pass nll backward?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EffzDFkqMX"
      },
      "source": [
        "## Activation and Loss Test Cases: ##\n",
        "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DJFzpahkvcD",
        "outputId": "5a2fe4b9-fd9a-47a3-f9d4-13c3413c70c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
        "np.random.seed(0)\n",
        "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear_1.W: OK\n",
            "linear_1.W0: OK\n",
            "linear_2.W: OK\n",
            "linear_2.W0: OK\n",
            "z_1: OK\n",
            "a_1: OK\n",
            "z_2: OK\n",
            "a_2: OK\n",
            "loss: OK\n",
            "dloss: OK\n",
            "dL_dz2: OK\n",
            "dL_da1: OK\n",
            "dL_dz1: OK\n",
            "dL_dX: OK\n",
            "updated_linear_1.W: OK\n",
            "updated_linear_1.W0: OK\n",
            "updated_linear_2.W: OK\n",
            "updated_linear_2.W0: OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in arctanh\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd0dXg-Qk05_"
      },
      "source": [
        "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
        "#np.random.seed(0)\n",
        "#sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l5JgBU2iBCZ"
      },
      "source": [
        "## Neural Network: ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXMGcdnXgiF3"
      },
      "source": [
        "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
        "\n",
        "Hint: First call `Sequential.loss.backward()` to get dLdZ (in the case of the NLL loss function) before calling `Sequential.backward` with dLdZ as your input to propagate the loss backward through the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejO15Vr7fhKB"
      },
      "source": [
        " class Sequential:\n",
        "    def __init__(self, modules, loss):            # List of modules, loss module\n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
        "        D, N = X.shape\n",
        "        sum_loss = 0\n",
        "        for it in range(iters):\n",
        "            rnd = np.random.randint(N)\n",
        "            Xt, Yt =  X[:,rnd:rnd+1], Y[:,rnd:rnd+1]\n",
        "            Ypred = self.forward(Xt)\n",
        "            sum_loss += self.loss.forward(Ypred, Yt)\n",
        "            dLdZ = self.loss.backward()\n",
        "            self.backward(dLdZ)\n",
        "            self.sgd_step(lrate)\n",
        "\n",
        "    def forward(self, Xt):                        # Compute Ypred\n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
        "        # Note reversed list of modules\n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def sgd_step(self, lrate):                    # Gradient descent step\n",
        "        for m in self.modules: m.sgd_step(lrate)\n",
        "\n",
        "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
        "        # Utility method to print accuracy on full dataset, should\n",
        "        # improve over time when doing SGD. Also prints current loss,\n",
        "        # which should decrease over time. Call this on each iteration\n",
        "        # of SGD!\n",
        "        if it % every == 1:\n",
        "            cf = self.modules[-1].class_fun\n",
        "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
        "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUojaXqphDjh"
      },
      "source": [
        "## Neural Network / SGD Test Cases: ##\n",
        "Use Test 3 to help you debug."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaWfgC7Qe3ar"
      },
      "source": [
        "# TEST 3: try calling these methods that train with a simple dataset\n",
        "def nn_tanh_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
        "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
        "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
        "\n",
        "\n",
        "def nn_relu_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
        "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
        "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
        "\n",
        "\n",
        "def nn_pred_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
        "    Ypred = nn.forward(X)\n",
        "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dx-zM2y3R0z",
        "outputId": "08683862-2beb-484d-c0dc-ea472ad02dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "nn_tanh_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
        "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
        "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
        "#  [[0.544808855557535, -0.08366117689965663],\n",
        "#   [-0.06331837550937104, 0.24078409926389266],\n",
        "#   [0.08677202043839037, 0.8360167748667923],\n",
        "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
        "# '''"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
              "  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
              "  [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
              " [[0.544808855557535, -0.08366117689965663],\n",
              "  [-0.06331837550937104, 0.24078409926389266],\n",
              "  [0.08677202043839037, 0.8360167748667923],\n",
              "  [-0.0037249480614718, 0.0037249480614718]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmYT9IWk3TQL",
        "outputId": "f758e51c-126c-4a43-ad41-4b4e765362b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "nn_relu_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
        "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
        "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
        "#  [[0.501769700845158, -0.040622022187279644],\n",
        "#   [-0.09260786974986723, 0.27007359350438886],\n",
        "#   [0.08364438851530624, 0.8391444067898763],\n",
        "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
        "# '''"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[1.2422381908871343, 0.2850917905905376, 0.6979154428450828],\n",
              "  [1.5695971237335808, 1.32736695881922, -0.6734285638518853],\n",
              "  [-0.0027599281164093696, 0.0012016167968404868, 0.003194616722684224]],\n",
              " [[0.5019699887733605, -0.040822310115482194],\n",
              "  [-0.09250521279479793, 0.2699709365493196],\n",
              "  [0.08371056534069252, 0.8390782299644901],\n",
              "  [-0.0042231049094530945, 0.004223104909453096]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo_woDFh3a2v",
        "outputId": "2ce7d2f1-49be-4298-c8f7-006edab386db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nn_pred_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# ([0, 0, 0, 0], [8.56575061835767])\n",
        "# '''"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 0, 0, 0], [8.435824137827966])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    }
  ]
}