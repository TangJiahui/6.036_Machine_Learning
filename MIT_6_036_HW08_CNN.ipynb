{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT6.036 Fall 2020 Hw 08 Colab Notebook",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9efbac1fe5b458ebd179a2b4a9a9515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0352462be63d4da9960ecc4be43b3a5c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e897649130eb4fcf95b8e141edafd2bb",
              "IPY_MODEL_41eddd7ce9c445d79608791e1727a5a2"
            ]
          }
        },
        "0352462be63d4da9960ecc4be43b3a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e897649130eb4fcf95b8e141edafd2bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5de22933be6e4b349f0192228706ebf3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e6128be59a1a48858f65152246dfb879"
          }
        },
        "41eddd7ce9c445d79608791e1727a5a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d386dc2f6c9d421aa0dfb765c32c6e16",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:00&lt;00:00, 13152549.76it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15a388194c034446b05505e45a6e0a4d"
          }
        },
        "5de22933be6e4b349f0192228706ebf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e6128be59a1a48858f65152246dfb879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d386dc2f6c9d421aa0dfb765c32c6e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15a388194c034446b05505e45a6e0a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86fa5dfdb1b3454288c4d0e44787000e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_221d42c689b54e67b583cb162256da52",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e0aec35ed33e4f5f9258a4e961525d0c",
              "IPY_MODEL_2b5939c97612423ead8222a7a6deced3"
            ]
          }
        },
        "221d42c689b54e67b583cb162256da52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0aec35ed33e4f5f9258a4e961525d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f48d8ed7d86b4aeb83bf0f5b748bd222",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ab8a43826264a6e965e8dace7be6b00"
          }
        },
        "2b5939c97612423ead8222a7a6deced3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5634b028a67d4395b5b9a0d765f9c9e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:01&lt;00:00, 21008.31it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dea511217f8e49fb8f9958e03d3c1a7c"
          }
        },
        "f48d8ed7d86b4aeb83bf0f5b748bd222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ab8a43826264a6e965e8dace7be6b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5634b028a67d4395b5b9a0d765f9c9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dea511217f8e49fb8f9958e03d3c1a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50fdc515cac04020ad24e9ed1c89df0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_61fdd9702b4e4b7ca6da0ad9dc4bdd31",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_781b50aa9a5e470b934700a1dc52642f",
              "IPY_MODEL_705d4967c1274453b0bc281c0edb287d"
            ]
          }
        },
        "61fdd9702b4e4b7ca6da0ad9dc4bdd31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "781b50aa9a5e470b934700a1dc52642f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8bdebe873b6a42979863a3c6827fca75",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fb99ebaff5c42a1988b5f6d6c98af30"
          }
        },
        "705d4967c1274453b0bc281c0edb287d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b240b6a212aa4e0b8550c07baf491410",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:00&lt;00:00, 2154247.67it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fac615aeaaac4de9b1f8762cfd323d89"
          }
        },
        "8bdebe873b6a42979863a3c6827fca75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fb99ebaff5c42a1988b5f6d6c98af30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b240b6a212aa4e0b8550c07baf491410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fac615aeaaac4de9b1f8762cfd323d89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b63cb234b99f47469fd41479e4df3903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4379a90245fc410dac8b82751a899f72",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9feaf212a1004dc09ddcae2e619ec911",
              "IPY_MODEL_49e3d6a35a734f268161c28a9e4c79e8"
            ]
          }
        },
        "4379a90245fc410dac8b82751a899f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9feaf212a1004dc09ddcae2e619ec911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9ebf3365f7443ab8229ea8f30d19f0e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_106906d7790a4ffb93218d45f17ff2b7"
          }
        },
        "49e3d6a35a734f268161c28a9e4c79e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_21e4a24d17724a77a99bc63d4e73b089",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 16756.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_04e126b5641e475bb0ea3c6b3aef5195"
          }
        },
        "f9ebf3365f7443ab8229ea8f30d19f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "106906d7790a4ffb93218d45f17ff2b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21e4a24d17724a77a99bc63d4e73b089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "04e126b5641e475bb0ea3c6b3aef5195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TangJiahui/6.036_Machine_Learning/blob/main/MIT_6_036_HW08_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A"
      },
      "source": [
        "#MIT 6.036 Fall 2020: Homework 8#\n",
        "\n",
        "This colab notebook provides code and a framework for [homework 8](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Fall/courseware/Week8/week8_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-S-l98HBVoa",
        "outputId": "a945fa66-ca1f-491c-a7f5-b052d33361bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!rm -rf code_for_hw8*\n",
        "!rm -rf data\n",
        "!rm -rf mnist_data\n",
        "!rm -rf *.zip\n",
        "!rm -rf test*/\n",
        "!rm -rf *.py\n",
        "!rm -rf *.pt\n",
        "!rm -rf __*\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw08/code_for_hw8.zip --no-check-certificate\n",
        "!unzip code_for_hw8.zip\n",
        "!unzip code_for_hw8/q4.zip\n",
        "!unzip -q test1.zip\n",
        "!unzip -q test2.zip\n",
        "!unzip -q test3.zip\n",
        "!mv code_for_hw8/* .\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "import math as m\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision\n",
        "\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "\n",
        "import os\n",
        "\n",
        "from code_for_hw8_oop import Module, Linear, Tanh, ReLU, SoftMax, NLL\n",
        "from code_for_hw8_pytorch import get_image_data_1d\n",
        "\n",
        "from utils_hw8 import (model_fit, model_evaluate, run_pytorch, call_model, \n",
        "                       plot_decision, plot_heat, plot_separator, make_iter, \n",
        "                       set_weights, set_bias)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_hw8.zip\n",
            "  inflating: code_for_hw8/code_for_hw8_oop.py  \n",
            "  inflating: code_for_hw8/code_for_hw8_pytorch.py  \n",
            "   creating: code_for_hw8/data/\n",
            "  inflating: code_for_hw8/data/data1_train.csv  \n",
            "  inflating: code_for_hw8/data/data1_validate.csv  \n",
            "  inflating: code_for_hw8/data/data2_train.csv  \n",
            "  inflating: code_for_hw8/data/data2_validate.csv  \n",
            "  inflating: code_for_hw8/data/data3_train.csv  \n",
            "  inflating: code_for_hw8/data/data3_validate.csv  \n",
            "  inflating: code_for_hw8/data/data3class_train.csv  \n",
            "  inflating: code_for_hw8/data/data4_train.csv  \n",
            "  inflating: code_for_hw8/data/data4_validate.csv  \n",
            "  inflating: code_for_hw8/data/dataXor_train.csv  \n",
            " extracting: code_for_hw8/q4.zip     \n",
            "  inflating: code_for_hw8/utils_hw8.py  \n",
            "Archive:  code_for_hw8/q4.zip\n",
            "  inflating: test3.zip               \n",
            "  inflating: code_for_hw8_q4.py      \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._code_for_hw8_q4.py  \n",
            "  inflating: test2.zip               \n",
            "  inflating: test1.zip               \n",
            "  inflating: squeezenet_trained_cats_v_dogs.pt  \n",
            "  inflating: __MACOSX/._squeezenet_trained_cats_v_dogs.pt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-sSs7N4mMiX"
      },
      "source": [
        "# 1) Implementing Mini-batch Gradient Descent and Batch Normalization\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "Last week we implemented a framework for building neural networks from scratch. We trained our models using *stochastic* gradient descent. In this problem, we explore how we can implement batch normalization as a module `BatchNorm` in our framework. It is the same module which you analyzed in problem 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgxmIfXVmVwd"
      },
      "source": [
        "Key to the concept of batch normalization is the doing gradient descent on batches of data. So we instead of using last week's stochastic gradient descent, we will first implement the *mini-batch* gradient descent method `mini_gd`, which is a hybrid between *stochastic* gradient descent and *batch* gradient descent. The lecture notes on <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/neural_networks_2/1?activate_block_id=block-v1%3AMITx%2B6.036%2B2019_Spring%2Btype%40vertical%2Bblock%40neural_networks_2_optimizing_neural_network_parameters_vert\"> optimizing neural network parameters</a> are helpful for this part.\n",
        "\n",
        "In *mini-batch* gradient descent, for a mini-batch of size $K$, we select $K$ distinct data points uniformly at random from the data set and update the network weights based only on their contributions to the gradient:\n",
        "$$W := W - \\eta\\sum_{i=1}^K \\nabla_W \\mathcal{L}(h(x^{(i)}; W), y^{(i)})\\;\\;.$$\n",
        "\n",
        "Our *mini-batch* method `mini_gd` will be implemented within the `Sequential` python class (see homework 7 problem 2) and will take the following as inputs:\n",
        "\n",
        "* `X`: a standard data array (d by n)\n",
        "* `y`: a standard labels row vector (1 by n)\n",
        "* `iters`: the number of updates to perform on weights $W$\n",
        "* `lrate`: the learning rate used\n",
        "* `K`: the mini-batch size to be used\n",
        "\n",
        "One call of `mini_gd` should call `Sequential.backward` for back-propagation and `Sequential.step` for updating the weights, for a total of `iters` times, using `lrate` as the learning rate. As in our implementation of `sgd` from homework 7, we compute the predicted output for a mini-batch of data with the `Sequential.forward` method. We compute the loss between our predictions and the true labels using the assigned `Sequential.loss` method. (Note that in homework 7, `Sequential.step` was called `Sequential.sgd_step`. While the functionality of the step function is the same, it has been renamed for convenience. The same is true for the `module.step` function of each module we implemented, where applicable.)\n",
        "\n",
        "For picking $K$ unique data points at random from our large data-set for each mini-batch, we will implement the following strategy: we will first shuffle our data points `X` (and associated labels `y`). Then, we get $\\frac{n}{k}$ (rounded down to the nearest integer) different mini-batches by grouping each $K$ consecutive points from this shuffled array. If we end up iterating over all the points but need more mini-batches, we will repeat the shuffling and the batching process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr1kWI08mdo4"
      },
      "source": [
        "<b>1A)</b> You need to fill in the missing code below. We have implemented the shuffling of indices and have provided you with the outer and inner loops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHso5_RFrgnF"
      },
      "source": [
        "Implement `mini_gd` in `Sequential` below.\n",
        "\n",
        "**Hint:** The documentation for <a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.shuffle.html\"> `numpy.random.shuffle`</a> might be helpful for this part. If you have a list of elements `l` and a set of indices `indices`, you can call `numpy.random.shuffle(indices)` and `l = l[indices]` to shuffle the elements of `l`.\n",
        "\n",
        "**Implementation Note:** “Notice that y is represented as Y which is one-hot encoded”\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brq8yO4trfCb"
      },
      "source": [
        "import math\n",
        "class Sequential:\n",
        "    def __init__(self, modules, loss):            \n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def mini_gd(self, X, Y, iters, lrate, notif_each=None, K=10):\n",
        "        D, N = X.shape\n",
        "\n",
        "        np.random.seed(0)\n",
        "        num_updates = 0\n",
        "        indices = np.arange(N)\n",
        "        while num_updates < iters:\n",
        "\n",
        "            np.random.shuffle(indices)\n",
        "            X = X[:, indices]\n",
        "            Y = Y[:, indices]\n",
        "\n",
        "            for j in range(math.floor(N/K)):\n",
        "                if num_updates >= iters: break\n",
        "\n",
        "                # Implement the main part of mini_gd here\n",
        "                Xt = X[:,j*K:(j+1)*K]  \n",
        "                Yt = Y[:,j*K:(j+1)*K]  \n",
        "                Ypred = self.forward(Xt)\n",
        "                loss = self.loss.forward(Ypred, Yt)\n",
        "                dLdZ = self.loss.backward()\n",
        "                self.backward(dLdZ)\n",
        "                self.sgd_step(lrate)    \n",
        "                num_updates += 1\n",
        "\n",
        "    def forward(self, Xt):                        \n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):                   \n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def sgd_step(self, lrate):    \n",
        "        for m in self.modules: m.sgd_step(lrate)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JZeeKXkm6YI"
      },
      "source": [
        "<b>1B)</b> We are now ready to implement batch normalization into our neural network framework! Our module `BatchNorm` will sit between consecutive layers of neurons, such as the $l^{th}$ and $(l+1)^{th}$ layers, acting as a \"corrector\" which allows $W^l$ to change freely, producing outputs $z^l$, but then the module corrects the covariate shift induced in the signals before they reach the $(l+1)^{th}$ layer, converting $z^l$ to $\\widehat{Z}^l$. \n",
        "\n",
        "The following is a summmary what is described in the <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/neural_networks_2/2\">lecture notes</a>, and it should guide your implementation of the module. \n",
        "\n",
        "Any normalization between the $l^{th}$ and $(l+1)^{th}$ layers is done *separately* for each of the $n^l$ input connections leading to the $(l+1)^{th}$ layer. We handle a mini-batch of data of size $K$, and $Z^l$ is $n^l \\times K$, and the output $\\widehat{Z}^l$is of the same shape. \n",
        "\n",
        "We first compute $n^l$ *batchwise* means and\n",
        "standard deviations.  Let $\\mu^l$ be the $n^l \\times 1$ vector (`self.mus`) where\n",
        "$$\\mu^l_i = \\frac{1}{K} \\sum_{j = 1}^K Z^l_{ij}\\;\\;,$$\n",
        "and let $\\sigma^l$ be the $n^l \\times 1$ vector of standard deviations where \n",
        "$$\\sigma^l_i = \\sqrt{\\frac{1}{K} \\sum_{j = 1}^K (Z^l_{ij} - \\mu_i)^2}\\;\\;.$$\n",
        "Note that `self.vars` is the variance, or element-wise square of  $\\sigma^l_i$.\n",
        "\n",
        "The normalized data `self.norm` is the matrix $\\overline{Z}$, where\n",
        "$$\\overline{Z}^l_{ij} = \\frac{Z^l_{ij} - \\mu^l_i}{\\sigma^l_i + \\epsilon}\\;\\;,$$\n",
        "and where $\\epsilon$ is a very small constant to guard against division by\n",
        "zero. \n",
        "\n",
        "We define weights $G^l$ (`self.G`) and $B^l$ (`self.B`), each being an $n^l \\times 1$ vector, which we use to to shift and scale the outputs:\n",
        "$$\\widehat{Z}^l_{ij} = G^l_i \\overline{Z}^l_{ij} + B^l_i\\;\\;.$$\n",
        "\n",
        "The outputs are finally ready to be passed to the $(l+1)^{th}$ layer.\n",
        "\n",
        "A slight warning (that we will not worry about here) about `BatchNorm` is that during the *test* phase, if the test mini-batch size is too small (imagine we are deploying a neural network that deals with live video frames), then the lack of samples would cause the freshly-calculated $\\mu^l$ and $\\sigma^l$ to be far off from their true values that the module's parameters $G^l$ and $B^l$ were trained to be compatible with. To fix that, people usually compute a running average of $\\mu^l$ and $\\sigma^l$ during training, to be used at test time. We will assume our test mini-batches are large enough.\n",
        "\n",
        "In this problem we only implement the `BatchNorm.forward` and `BatchNorm.sgd_step` methods. We provide you with the implementation for `BatchNorm.backward` and the lecture notes contain the details of the derivations. You will need to fill in the missing code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlXP26plm8R7"
      },
      "source": [
        "class BatchNorm(Module):    \n",
        "    def __init__(self, m):\n",
        "        np.random.seed(0)\n",
        "        self.eps = 1e-20\n",
        "        self.m = m  # number of input channels\n",
        "        \n",
        "        # Init learned shifts and scaling factors\n",
        "        self.B = np.zeros([self.m, 1]) # m x 1\n",
        "        self.G = np.random.normal(0, 1.0 * self.m ** (-.5), [self.m, 1]) # m x 1\n",
        "        \n",
        "    def forward(self, Z):\n",
        "        # Z is n^l x K: m input channels and mini-batch size K\n",
        "        # Store last inputs and K for next backward() call\n",
        "        self.Z = Z\n",
        "        self.K = Z.shape[1]\n",
        "        \n",
        "        self.mus = np.mean(Z, axis=1, keepdims=True)\n",
        "        self.vars = np.var(Z, axis=1, keepdims=True)\n",
        "\n",
        "        # Normalize inputs using their mean and standard deviation\n",
        "        self.norm = (Z - self.mus)/(np.sqrt(self.vars) + self.eps)\n",
        "            \n",
        "        # Return scaled and shifted versions of self.norm\n",
        "        return self.G * self.norm + self.B\n",
        "\n",
        "    def backward(self, dLdZ):\n",
        "        # Re-usable constants\n",
        "        std_inv = 1/np.sqrt(self.vars+self.eps)\n",
        "        Z_min_mu = self.Z-self.mus\n",
        "        \n",
        "        dLdnorm = dLdZ * self.G\n",
        "        dLdVar = np.sum(dLdnorm * Z_min_mu * -0.5 * std_inv**3, axis=1, keepdims=True)\n",
        "        dLdMu = np.sum(dLdnorm*(-std_inv), axis=1, keepdims=True) + dLdVar * (-2/self.K) * np.sum(Z_min_mu, axis=1, keepdims=True)\n",
        "        dLdX = (dLdnorm * std_inv) + (dLdVar * (2/self.K) * Z_min_mu) + (dLdMu/self.K)\n",
        "        \n",
        "        self.dLdB = np.sum(dLdZ, axis=1, keepdims=True)\n",
        "        self.dLdG = np.sum(dLdZ * self.norm, axis=1, keepdims=True)\n",
        "        return dLdX\n",
        "\n",
        "    def sgd_step(self, lrate):\n",
        "        self.B = self.B - lrate*self.dLdB\n",
        "        self.G = self.G - lrate*self.dLdG"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4u39OCjLza"
      },
      "source": [
        "# 2) Weight sharing (OPTIONAL)\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "In the lab we designed a CNN that can count the number of objects in 1 dimensional images, where each black pixel is represented by a value of 0 and each white pixel is represented by a value of 1. Recall that an object is a consecutive sequence of black pixels ($0$'s). For example, the sequence $0100110$ contains three objects. \n",
        "\n",
        "In this problem we want to see how hard/easy it is to train such a network from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp8stejLA57F"
      },
      "source": [
        "\n",
        "Our network architecture will be as follows:\n",
        "\n",
        "*    The first layer is convolutional and you will implement it using the PyTorch `torch.nn.Conv1d` function, with a kernel of size 2 and stride of 1, followed by a ReLu activation (`torch.nn.ReLU`).\n",
        "\n",
        "*    The second layer is a fully connected `torch.nn.Linear` layer which has a scalar output.\n",
        "\n",
        "Here is sample usage of the `Conv1d` and `Linear` layers. \n",
        "\n",
        "`layer1=torch.nn.Conv1d(in_channels=?, out_channels=?, kernel_size=?,stride=?,padding=?,bias=True)`\n",
        "\n",
        "Here, `in_channels` is the number of channels in your data (so for example, RGB images have 3 channels). You can think of the `out_channels` variable as the number of filters you are using.\n",
        "\n",
        "`layer3 = torch.nn.Linear(in_units=?, out_units=?)`\n",
        "\n",
        "You need to fill in the parameters marked with `?` based on the problem specifications. Note also that in PyTorch, depending on your implementation, you may be forced to use *three* (four if we count ReLU) layers to implement such a network, where one intermediary `Flatten` layer is used to flatten the output of the convolutional layer, before being passed to the dense layer.\n",
        "\n",
        "Refer to the <a href=\"https://pytorch.org/docs/stable/nn.html#conv1d\">Conv1D</a>, <a href=\"https://pytorch.org/docs/stable/nn.html#linear\">Linear</a> and <a href=\"https://pytorch.org/docs/stable/nn.html#flatten\">Flatten</a> descriptions in the PyTorch documentation to see the available parameter options.\n",
        "\n",
        "In this exercise, we fix the structure and want to learn the best combination of weights from data. In the homework code, we have provided functions `train_neural_counter` and `get_image_data_1d`. You can use them to generate data and train the above neural network in PyTorch to answer the following questions. We assume that the images in our data set are randomly generated. The probability of a pixel being white is $0.1$. We work with mean squared error as the loss function for this problem. We have provided template code which you can fill in, to perform the training.\n",
        "\n",
        "We have also provided helper functions such as `set_weight`, `set_bias`, which might help you set weights and biases of a particular layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKa8iMv_j3ek"
      },
      "source": [
        "<b>2B)</b> What is (approximately) the expected loss of the network on $1024\\times 1$ images if the convolutional layer is an averaging filter and second layer is the sum function (without a bias term)? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPcB588ok8a",
        "outputId": "1cf10677-4089-4de4-af29-2b23d1a3f9f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Code template if you would like to check 2B) through code\n",
        "\n",
        "tsize = 1000\n",
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "(X_train,Y_train,X_val,Y_val,X_test,Y_test) = get_image_data_1d(tsize,imsize,prob_white)\n",
        "test_loader = make_iter(X_test, Y_test)\n",
        "\n",
        "num_filters = 1\n",
        "kernel_size = 2\n",
        "strides = 1\n",
        "padding = 1\n",
        "\n",
        "layer_1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, stride=strides, padding=padding, bias=False)\n",
        "\n",
        "num_units = imsize+1  # Your code\n",
        "layer_3 = nn.Linear(num_units, 1, bias=False)\n",
        "layers = [layer_1, nn.ReLU(), nn.Flatten(), layer_3]\n",
        "model = nn.Sequential(*layers)\n",
        "\n",
        "set_weights(model[0], np.array([1/2,1/2]))\n",
        "set_weights(model[-1], np.ones(num_units))\n",
        "\n",
        "model_evaluate(model, test_loader, nn.MSELoss())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97.531, 97.531)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3OYsbwj7Ms"
      },
      "source": [
        "<b>2C)</b> Now suppose we add a bias term of $-10$ to the last layer. What is (approximately) the expected quadratic loss? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKynxhF1klga",
        "outputId": "3c6ce777-fdcb-46bb-f3cd-b7c57cd4bbeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Edit code from 2B) with the bias\n",
        "bias = -10\n",
        "set_bias(model[-1], bias)\n",
        "model_evaluate(model, test_loader, nn.MSELoss())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12.631, 12.631)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCLr8qmj-Hk"
      },
      "source": [
        "<b>2D)</b> Averaging type filters are abundant and form a nearly flat valley of local minima for this problem. It is difficult for the network to find alternative solutions on its own. We need to force our way out of these bad minima and towards a better solution, i.e., an edge detector. To force the first layer to behave as an edge detector, we need to choose a proper **kernel regularizer**. Consider the following functions\n",
        "\n",
        "$f_1=\\sum_i |w_i|$, $f_2=\\sum_i |w_i^2|$, $f_3=|\\sum_{i} w_i|$. Which one of the choices is likely to guide the network to find an edge detector at the convolution layer?\n",
        "\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Fall/courseware/Week8/week8_homework/\">Refer to HW8 on MITx.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aubU6Q6kwOI"
      },
      "source": [
        "Implement your choice of regularizers from above in the code (complete the function `filter_reg`). Do not allow any bias in the layers for the rest of the problem. The code generates some random test and training data sets and trains the model on these data. Run a few learning trials (5 or more) for each data set and answer the following questions based on the performance of your model.\n",
        "\n",
        "**IMPORTANT**: When implementing `filter_reg`, you should use the torch backend operations, imported as \"torch\" in the code. So for example, `torch.sum` and `torch.abs`, rather than `np.sum` and `np.abs`. This is because the `weights` argument is NOT a numpy object, but rather an internal torch object!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOLZf_JsuTLn"
      },
      "source": [
        "# Implement filter_reg\n",
        "def filter_reg(weights, lam = 1000):\n",
        "    # We scale the output of the filter by lam\n",
        "    lam=lam\n",
        "    filter_result = torch.abs(torch.sum(weights))\n",
        "    return lam * filter_result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bXvcRuk_3q0"
      },
      "source": [
        "def model_reg(model):\n",
        "    # Don't edit this function!\n",
        "    filter_weights = model[0].weight\n",
        "    return filter_reg(filter_weights)\n",
        "\n",
        "def train_neural_counter(layers, data, regularize=False, display=False):\n",
        "    (X_train, Y_train, X_val, Y_val, X_test, Y_test) = data\n",
        "    epochs = 10\n",
        "    batch = 1\n",
        "\n",
        "    train_iter, val_iter, test_iter = (make_iter(X_train, Y_train),\n",
        "                                       make_iter(X_val,Y_val),\n",
        "                                       make_iter(X_test,Y_test))\n",
        "    model = nn.Sequential(*layers)\n",
        "    optimizer = Adam(model.parameters())\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    model_fit(model, train_iter, epochs, optimizer, criterion, val_iter, \n",
        "              history=None,verbose=True, model_reg=model_reg if regularize else None)\n",
        "    err = model_evaluate(model, test_iter, criterion)\n",
        "    ws = model[-1].weight\n",
        "    if display:\n",
        "        plt.plot(ws)\n",
        "        plt.show()\n",
        "    return model,err"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRwd0eJkAdh"
      },
      "source": [
        "<b>2E)</b> For $1024\\times 1$ images and training set of size $1000$, is the network **without any regularization** likely to find models that have a mean square error lower than 8 on the test data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KiCbZmksXO6",
        "outputId": "7bfd84ab-6e56-40e2-b153-395145eaf941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "data=get_image_data_1d(1000, imsize, prob_white)\n",
        "trials=5\n",
        "\n",
        "\n",
        "for trial in range(trials):\n",
        "    num_filters = 1\n",
        "    kernel_size = 2\n",
        "    strides = 1\n",
        "    padding = 1\n",
        "\n",
        "    layer_1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, stride=strides, padding=padding, bias=False)\n",
        "\n",
        "    num_units = imsize+1\n",
        "    layer_3 = nn.Linear(num_units, 1, bias=False)\n",
        "    layers = [layer_1, nn.ReLU(), nn.Flatten(), layer_3]\n",
        "    model, err=train_neural_counter(layers, data)\n",
        "    print(model[0].weight)\n",
        "    print(torch.mean(model[-1].weight))\n",
        "    print(err)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 3600.98605 acc 3600.98605 | VALI: loss 20.717 acc 20.717\n",
            "epoch: 1 | TRAIN: loss 10.20671 acc 10.20671 | VALI: loss 9.03121 acc 9.03121\n",
            "epoch: 2 | TRAIN: loss 9.06348 acc 9.06348 | VALI: loss 9.01859 acc 9.01859\n",
            "epoch: 3 | TRAIN: loss 8.45391 acc 8.45391 | VALI: loss 9.07445 acc 9.07445\n",
            "epoch: 4 | TRAIN: loss 7.73103 acc 7.73103 | VALI: loss 9.22948 acc 9.22948\n",
            "epoch: 5 | TRAIN: loss 6.99754 acc 6.99754 | VALI: loss 9.49843 acc 9.49843\n",
            "epoch: 6 | TRAIN: loss 6.33614 acc 6.33614 | VALI: loss 9.86022 acc 9.86022\n",
            "epoch: 7 | TRAIN: loss 5.78112 acc 5.78112 | VALI: loss 10.27603 acc 10.27603\n",
            "epoch: 8 | TRAIN: loss 5.33021 acc 5.33021 | VALI: loss 10.71341 acc 10.71341\n",
            "epoch: 9 | TRAIN: loss 4.96538 acc 4.96538 | VALI: loss 11.15183 acc 11.15183\n",
            "Parameter containing:\n",
            "tensor([[[1.0306, 1.3024]]], requires_grad=True)\n",
            "tensor(0.3897, grad_fn=<MeanBackward0>)\n",
            "(12.996720126522066, 12.996720126522066)\n",
            "epoch: 0 | TRAIN: loss 3217.45817 acc 3217.45817 | VALI: loss 16.34169 acc 16.34169\n",
            "epoch: 1 | TRAIN: loss 9.89328 acc 9.89328 | VALI: loss 9.03958 acc 9.03958\n",
            "epoch: 2 | TRAIN: loss 8.98573 acc 8.98573 | VALI: loss 9.03795 acc 9.03795\n",
            "epoch: 3 | TRAIN: loss 8.3712 acc 8.3712 | VALI: loss 9.10608 acc 9.10608\n",
            "epoch: 4 | TRAIN: loss 7.64879 acc 7.64879 | VALI: loss 9.28403 acc 9.28403\n",
            "epoch: 5 | TRAIN: loss 6.92385 acc 6.92385 | VALI: loss 9.58673 acc 9.58673\n",
            "epoch: 6 | TRAIN: loss 6.27614 acc 6.27614 | VALI: loss 9.98568 acc 9.98568\n",
            "epoch: 7 | TRAIN: loss 5.7355 acc 5.7355 | VALI: loss 10.43695 acc 10.43695\n",
            "epoch: 8 | TRAIN: loss 5.29777 acc 5.29777 | VALI: loss 10.90564 acc 10.90564\n",
            "epoch: 9 | TRAIN: loss 4.94468 acc 4.94468 | VALI: loss 11.3702 acc 11.3702\n",
            "Parameter containing:\n",
            "tensor([[[1.3574, 1.1702]]], requires_grad=True)\n",
            "tensor(0.3599, grad_fn=<MeanBackward0>)\n",
            "(13.259141184601, 13.259141184601)\n",
            "epoch: 0 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 1 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 2 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 3 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 4 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 5 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 6 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 7 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 8 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 9 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "Parameter containing:\n",
            "tensor([[[-0.2077, -0.1390]]], requires_grad=True)\n",
            "tensor(-2.0386e-06, grad_fn=<MeanBackward0>)\n",
            "(8771.595, 8771.595)\n",
            "epoch: 0 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 1 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 2 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 3 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 4 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 5 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 6 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 7 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 8 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 9 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "Parameter containing:\n",
            "tensor([[[-0.4525, -0.3985]]], requires_grad=True)\n",
            "tensor(-0.0006, grad_fn=<MeanBackward0>)\n",
            "(8771.595, 8771.595)\n",
            "epoch: 0 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 1 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 2 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 3 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 4 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 5 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 6 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 7 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 8 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "epoch: 9 | TRAIN: loss 8670.66 acc 8670.66 | VALI: loss 8639.29 acc 8639.29\n",
            "Parameter containing:\n",
            "tensor([[[-0.4431, -0.2998]]], requires_grad=True)\n",
            "tensor(0.0004, grad_fn=<MeanBackward0>)\n",
            "(8771.595, 8771.595)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1vcUEL-vW9D"
      },
      "source": [
        "#### For parts F) to J), simply edit your code from E) with the necessary changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_25ygQJkD5F"
      },
      "source": [
        "<b>2F)</b> Repeat the same experiment, but now with the regularizer you implemented. Try different regularization parameters. Which choice of regularization parameter gives the best prediction results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNAChIqylIlt"
      },
      "source": [
        "# Edit code from 2E), using your regularization turned on in the train_neural_counter function.\n",
        "# Try setting the lambda parameter to different values in filter_reg\n",
        "# Set regularize=True\n",
        "\n",
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "data=get_image_data_1d(1000, imsize, prob_white)\n",
        "trials=5\n",
        "\n",
        "def run_nn():\n",
        "    for trial in range(3):\n",
        "        num_filters = 1\n",
        "        kernel_size = 2\n",
        "        strides = 1\n",
        "        padding = 1\n",
        "\n",
        "        layer_1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=kernel_size, stride=strides, padding=padding, bias=False)\n",
        "\n",
        "        num_units = imsize+1\n",
        "        layer_3 = nn.Linear(num_units, 1, bias=False)\n",
        "        layers = [layer_1, nn.ReLU(), nn.Flatten(), layer_3]\n",
        "        model, err=train_neural_counter(layers, data, regularize=True)\n",
        "        print(model[0].weight)\n",
        "        print(torch.mean(model[-1].weight))\n",
        "        print(err)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba1MHGDlkihv",
        "outputId": "ede7271c-ed6b-4139-b38e-7c3097d7b77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for lam in [0, 1, 10, 1000]:\n",
        "    def filter_reg(weights, lam = lam):\n",
        "        # We scale the output of the filter by lam\n",
        "        lam=lam\n",
        "        filter_result = torch.abs(torch.sum(weights))\n",
        "        return lam * filter_result\n",
        "    run_nn()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 3481.61377 acc 3481.61377 | VALI: loss 26.99267 acc 26.99267\n",
            "epoch: 1 | TRAIN: loss 10.99282 acc 10.99282 | VALI: loss 8.8075 acc 8.8075\n",
            "epoch: 2 | TRAIN: loss 9.58514 acc 9.58514 | VALI: loss 8.59137 acc 8.59137\n",
            "epoch: 3 | TRAIN: loss 8.88217 acc 8.88217 | VALI: loss 8.43136 acc 8.43136\n",
            "epoch: 4 | TRAIN: loss 8.0664 acc 8.0664 | VALI: loss 8.46517 acc 8.46517\n",
            "epoch: 5 | TRAIN: loss 7.2609 acc 7.2609 | VALI: loss 8.7386 acc 8.7386\n",
            "epoch: 6 | TRAIN: loss 6.54751 acc 6.54751 | VALI: loss 9.20253 acc 9.20253\n",
            "epoch: 7 | TRAIN: loss 5.95223 acc 5.95223 | VALI: loss 9.77328 acc 9.77328\n",
            "epoch: 8 | TRAIN: loss 5.4684 acc 5.4684 | VALI: loss 10.39182 acc 10.39182\n",
            "epoch: 9 | TRAIN: loss 5.07684 acc 5.07684 | VALI: loss 11.02635 acc 11.02635\n",
            "Parameter containing:\n",
            "tensor([[[1.7072, 1.0711]]], requires_grad=True)\n",
            "tensor(0.3269, grad_fn=<MeanBackward0>)\n",
            "(14.916322615417943, 14.916322615417943)\n",
            "epoch: 0 | TRAIN: loss 4006.31451 acc 4006.31451 | VALI: loss 51.79867 acc 51.79867\n",
            "epoch: 1 | TRAIN: loss 12.64674 acc 12.64674 | VALI: loss 9.00419 acc 9.00419\n",
            "epoch: 2 | TRAIN: loss 9.65625 acc 9.65625 | VALI: loss 8.78421 acc 8.78421\n",
            "epoch: 3 | TRAIN: loss 8.9701 acc 8.9701 | VALI: loss 8.59687 acc 8.59687\n",
            "epoch: 4 | TRAIN: loss 8.1537 acc 8.1537 | VALI: loss 8.58716 acc 8.58716\n",
            "epoch: 5 | TRAIN: loss 7.32651 acc 7.32651 | VALI: loss 8.8092 acc 8.8092\n",
            "epoch: 6 | TRAIN: loss 6.58107 acc 6.58107 | VALI: loss 9.22396 acc 9.22396\n",
            "epoch: 7 | TRAIN: loss 5.95551 acc 5.95551 | VALI: loss 9.75355 acc 9.75355\n",
            "epoch: 8 | TRAIN: loss 5.44748 acc 5.44748 | VALI: loss 10.33938 acc 10.33938\n",
            "epoch: 9 | TRAIN: loss 5.03714 acc 5.03714 | VALI: loss 10.94875 acc 10.94875\n",
            "Parameter containing:\n",
            "tensor([[[1.6797, 0.9421]]], requires_grad=True)\n",
            "tensor(0.3465, grad_fn=<MeanBackward0>)\n",
            "(14.830183501497144, 14.830183501497144)\n",
            "epoch: 0 | TRAIN: loss 4813.8857 acc 4813.8857 | VALI: loss 155.61328 acc 155.61328\n",
            "epoch: 1 | TRAIN: loss 20.27579 acc 20.27579 | VALI: loss 9.21716 acc 9.21716\n",
            "epoch: 2 | TRAIN: loss 9.76385 acc 9.76385 | VALI: loss 9.0244 acc 9.0244\n",
            "epoch: 3 | TRAIN: loss 9.12688 acc 9.12688 | VALI: loss 8.81037 acc 8.81037\n",
            "epoch: 4 | TRAIN: loss 8.33278 acc 8.33278 | VALI: loss 8.72768 acc 8.72768\n",
            "epoch: 5 | TRAIN: loss 7.48932 acc 7.48932 | VALI: loss 8.85479 acc 8.85479\n",
            "epoch: 6 | TRAIN: loss 6.70068 acc 6.70068 | VALI: loss 9.18267 acc 9.18267\n",
            "epoch: 7 | TRAIN: loss 6.02506 acc 6.02506 | VALI: loss 9.64502 acc 9.64502\n",
            "epoch: 8 | TRAIN: loss 5.4722 acc 5.4722 | VALI: loss 10.18212 acc 10.18212\n",
            "epoch: 9 | TRAIN: loss 5.0255 acc 5.0255 | VALI: loss 10.7584 acc 10.7584\n",
            "Parameter containing:\n",
            "tensor([[[1.6364, 0.7682]]], requires_grad=True)\n",
            "tensor(0.3773, grad_fn=<MeanBackward0>)\n",
            "(14.341285469439462, 14.341285469439462)\n",
            "epoch: 0 | TRAIN: loss 3929.33276 acc 3927.66159 | VALI: loss 55.02622 acc 55.02622\n",
            "epoch: 1 | TRAIN: loss 15.61978 acc 12.85558 | VALI: loss 8.9826 acc 8.9826\n",
            "epoch: 2 | TRAIN: loss 12.35949 acc 9.58637 | VALI: loss 8.81525 acc 8.81525\n",
            "epoch: 3 | TRAIN: loss 11.66386 acc 8.89256 | VALI: loss 8.65246 acc 8.65246\n",
            "epoch: 4 | TRAIN: loss 10.83995 acc 8.07093 | VALI: loss 8.6378 acc 8.6378\n",
            "epoch: 5 | TRAIN: loss 10.0091 acc 7.24283 | VALI: loss 8.85145 acc 8.85145\n",
            "epoch: 6 | TRAIN: loss 9.26029 acc 6.49733 | VALI: loss 9.27551 acc 9.27551\n",
            "epoch: 7 | TRAIN: loss 8.62805 acc 5.86904 | VALI: loss 9.82921 acc 9.82921\n",
            "epoch: 8 | TRAIN: loss 8.11078 acc 5.35635 | VALI: loss 10.4452 acc 10.4452\n",
            "epoch: 9 | TRAIN: loss 7.69069 acc 4.94141 | VALI: loss 11.08591 acc 11.08591\n",
            "Parameter containing:\n",
            "tensor([[[0.9170, 1.8299]]], requires_grad=True)\n",
            "tensor(0.3304, grad_fn=<MeanBackward0>)\n",
            "(14.857082804132252, 14.857082804132252)\n",
            "epoch: 0 | TRAIN: loss 3844.33617 acc 3842.9735 | VALI: loss 29.67978 acc 29.67978\n",
            "epoch: 1 | TRAIN: loss 13.42438 acc 11.09956 | VALI: loss 8.87283 acc 8.87283\n",
            "epoch: 2 | TRAIN: loss 11.97764 acc 9.64862 | VALI: loss 8.66723 acc 8.66723\n",
            "epoch: 3 | TRAIN: loss 11.31258 acc 8.98492 | VALI: loss 8.49962 acc 8.49962\n",
            "epoch: 4 | TRAIN: loss 10.5291 acc 8.20306 | VALI: loss 8.50116 acc 8.50116\n",
            "epoch: 5 | TRAIN: loss 9.74176 acc 7.41751 | VALI: loss 8.71028 acc 8.71028\n",
            "epoch: 6 | TRAIN: loss 9.03643 acc 6.71407 | VALI: loss 9.07902 acc 9.07902\n",
            "epoch: 7 | TRAIN: loss 8.44788 acc 6.12754 | VALI: loss 9.53487 acc 9.53487\n",
            "epoch: 8 | TRAIN: loss 7.97177 acc 5.65364 | VALI: loss 10.02843 acc 10.02843\n",
            "epoch: 9 | TRAIN: loss 7.58763 acc 5.27194 | VALI: loss 10.53359 acc 10.53359\n",
            "Parameter containing:\n",
            "tensor([[[1.3435, 0.9724]]], requires_grad=True)\n",
            "tensor(0.3922, grad_fn=<MeanBackward0>)\n",
            "(14.360285151640651, 14.360285151640651)\n",
            "epoch: 0 | TRAIN: loss 2842.05285 acc 2839.84155 | VALI: loss 14.15066 acc 14.15066\n",
            "epoch: 1 | TRAIN: loss 13.1729 acc 10.23935 | VALI: loss 8.70697 acc 8.70697\n",
            "epoch: 2 | TRAIN: loss 12.44207 acc 9.50782 | VALI: loss 8.49771 acc 8.49771\n",
            "epoch: 3 | TRAIN: loss 11.7194 acc 8.78739 | VALI: loss 8.37396 acc 8.37396\n",
            "epoch: 4 | TRAIN: loss 10.91331 acc 7.98406 | VALI: loss 8.4688 acc 8.4688\n",
            "epoch: 5 | TRAIN: loss 10.14555 acc 7.21958 | VALI: loss 8.80533 acc 8.80533\n",
            "epoch: 6 | TRAIN: loss 9.48143 acc 6.55936 | VALI: loss 9.30769 acc 9.30769\n",
            "epoch: 7 | TRAIN: loss 8.93398 acc 6.0164 | VALI: loss 9.88752 acc 9.88752\n",
            "epoch: 8 | TRAIN: loss 8.4925 acc 5.57996 | VALI: loss 10.49125 acc 10.49125\n",
            "epoch: 9 | TRAIN: loss 8.13757 acc 5.23059 | VALI: loss 11.0923 acc 11.0923\n",
            "Parameter containing:\n",
            "tensor([[[1.4874, 1.4186]]], requires_grad=True)\n",
            "tensor(0.3129, grad_fn=<MeanBackward0>)\n",
            "(15.237621867809851, 15.237621867809851)\n",
            "epoch: 0 | TRAIN: loss 4125.8571 acc 4110.97225 | VALI: loss 65.86806 acc 65.86806\n",
            "epoch: 1 | TRAIN: loss 40.14799 acc 13.83536 | VALI: loss 8.73234 acc 8.73234\n",
            "epoch: 2 | TRAIN: loss 36.05845 acc 9.76063 | VALI: loss 8.52612 acc 8.52612\n",
            "epoch: 3 | TRAIN: loss 35.15501 acc 9.06238 | VALI: loss 8.34505 acc 8.34505\n",
            "epoch: 4 | TRAIN: loss 34.03351 acc 8.23659 | VALI: loss 8.33621 acc 8.33621\n",
            "epoch: 5 | TRAIN: loss 32.81397 acc 7.40364 | VALI: loss 8.54379 acc 8.54379\n",
            "epoch: 6 | TRAIN: loss 31.60306 acc 6.65254 | VALI: loss 8.92362 acc 8.92362\n",
            "epoch: 7 | TRAIN: loss 30.46159 acc 6.01905 | VALI: loss 9.39444 acc 9.39444\n",
            "epoch: 8 | TRAIN: loss 29.40915 acc 5.50123 | VALI: loss 9.89609 acc 9.89609\n",
            "epoch: 9 | TRAIN: loss 28.44179 acc 5.08049 | VALI: loss 10.39786 acc 10.39786\n",
            "Parameter containing:\n",
            "tensor([[[1.5793, 0.7303]]], requires_grad=True)\n",
            "tensor(0.3928, grad_fn=<MeanBackward0>)\n",
            "(14.137228747142363, 14.137228747142363)\n",
            "epoch: 0 | TRAIN: loss 3620.41379 acc 3605.37964 | VALI: loss 24.02633 acc 24.02633\n",
            "epoch: 1 | TRAIN: loss 34.80135 acc 10.87153 | VALI: loss 8.74386 acc 8.74386\n",
            "epoch: 2 | TRAIN: loss 33.54407 acc 9.69154 | VALI: loss 8.54176 acc 8.54176\n",
            "epoch: 3 | TRAIN: loss 32.69881 acc 9.03515 | VALI: loss 8.37138 acc 8.37138\n",
            "epoch: 4 | TRAIN: loss 31.66998 acc 8.27335 | VALI: loss 8.35357 acc 8.35357\n",
            "epoch: 5 | TRAIN: loss 30.57096 acc 7.51456 | VALI: loss 8.51989 acc 8.51989\n",
            "epoch: 6 | TRAIN: loss 29.49914 acc 6.83703 | VALI: loss 8.82169 acc 8.82169\n",
            "epoch: 7 | TRAIN: loss 28.50804 acc 6.27242 | VALI: loss 9.18886 acc 9.18886\n",
            "epoch: 8 | TRAIN: loss 27.60987 acc 5.81662 | VALI: loss 9.57457 acc 9.57457\n",
            "epoch: 9 | TRAIN: loss 26.79533 acc 5.4501 | VALI: loss 9.95521 acc 9.95521\n",
            "Parameter containing:\n",
            "tensor([[[0.8920, 1.2218]]], requires_grad=True)\n",
            "tensor(0.4296, grad_fn=<MeanBackward0>)\n",
            "(13.857500409160856, 13.857500409160856)\n",
            "epoch: 0 | TRAIN: loss 2733.27256 acc 2709.11 | VALI: loss 13.11864 acc 13.11864\n",
            "epoch: 1 | TRAIN: loss 41.38576 acc 10.36538 | VALI: loss 8.87169 acc 8.87169\n",
            "epoch: 2 | TRAIN: loss 40.47514 acc 9.63777 | VALI: loss 8.61882 acc 8.61882\n",
            "epoch: 3 | TRAIN: loss 39.40872 acc 8.88682 | VALI: loss 8.45622 acc 8.45622\n",
            "epoch: 4 | TRAIN: loss 38.15194 acc 8.0701 | VALI: loss 8.50015 acc 8.50015\n",
            "epoch: 5 | TRAIN: loss 36.83359 acc 7.30618 | VALI: loss 8.76031 acc 8.76031\n",
            "epoch: 6 | TRAIN: loss 35.54212 acc 6.65314 | VALI: loss 9.16168 acc 9.16168\n",
            "epoch: 7 | TRAIN: loss 34.32031 acc 6.11929 | VALI: loss 9.61821 acc 9.61821\n",
            "epoch: 8 | TRAIN: loss 33.18191 acc 5.69166 | VALI: loss 10.0766 acc 10.0766\n",
            "epoch: 9 | TRAIN: loss 32.12434 acc 5.35018 | VALI: loss 10.51193 acc 10.51193\n",
            "Parameter containing:\n",
            "tensor([[[1.2455, 1.3980]]], requires_grad=True)\n",
            "tensor(0.3436, grad_fn=<MeanBackward0>)\n",
            "(14.453640983582824, 14.453640983582824)\n",
            "epoch: 0 | TRAIN: loss 6336.38958 acc 6193.41241 | VALI: loss 2036.78769 acc 2036.78769\n",
            "epoch: 1 | TRAIN: loss 639.03928 acc 471.30512 | VALI: loss 9.6543 acc 9.6543\n",
            "epoch: 2 | TRAIN: loss 1.41695 acc 1.17388 | VALI: loss 0.69406 acc 0.69406\n",
            "epoch: 3 | TRAIN: loss 0.60682 acc 0.40682 | VALI: loss 0.6129 acc 0.6129\n",
            "epoch: 4 | TRAIN: loss 0.51801 acc 0.32326 | VALI: loss 0.53689 acc 0.53689\n",
            "epoch: 5 | TRAIN: loss 0.46726 acc 0.23061 | VALI: loss 0.45468 acc 0.45468\n",
            "epoch: 6 | TRAIN: loss 0.42604 acc 0.14636 | VALI: loss 0.35039 acc 0.35039\n",
            "epoch: 7 | TRAIN: loss 0.30016 acc 0.08833 | VALI: loss 0.29929 acc 0.29929\n",
            "epoch: 8 | TRAIN: loss 0.23243 acc 0.05537 | VALI: loss 0.23688 acc 0.23688\n",
            "epoch: 9 | TRAIN: loss 0.21562 acc 0.04018 | VALI: loss 0.20457 acc 0.20457\n",
            "Parameter containing:\n",
            "tensor([[[ 1.7942, -1.7942]]], requires_grad=True)\n",
            "tensor(0.5604, grad_fn=<MeanBackward0>)\n",
            "(0.19260938800801522, 0.19260938800801522)\n",
            "epoch: 0 | TRAIN: loss 6340.26196 acc 5881.71307 | VALI: loss 1707.81318 acc 1707.81318\n",
            "epoch: 1 | TRAIN: loss 797.35049 acc 397.40192 | VALI: loss 31.8561 acc 31.8561\n",
            "epoch: 2 | TRAIN: loss 3.77252 acc 3.47855 | VALI: loss 0.76144 acc 0.76144\n",
            "epoch: 3 | TRAIN: loss 0.67323 acc 0.40913 | VALI: loss 0.68666 acc 0.68666\n",
            "epoch: 4 | TRAIN: loss 0.58826 acc 0.32241 | VALI: loss 0.59049 acc 0.59049\n",
            "epoch: 5 | TRAIN: loss 0.49265 acc 0.22665 | VALI: loss 0.4977 acc 0.4977\n",
            "epoch: 6 | TRAIN: loss 0.40773 acc 0.14269 | VALI: loss 0.40978 acc 0.40978\n",
            "epoch: 7 | TRAIN: loss 0.32583 acc 0.08493 | VALI: loss 0.33296 acc 0.33296\n",
            "epoch: 8 | TRAIN: loss 0.31391 acc 0.05444 | VALI: loss 0.28156 acc 0.28156\n",
            "epoch: 9 | TRAIN: loss 0.29692 acc 0.04108 | VALI: loss 0.22596 acc 0.22596\n",
            "Parameter containing:\n",
            "tensor([[[ 1.9862, -1.9862]]], requires_grad=True)\n",
            "tensor(0.5060, grad_fn=<MeanBackward0>)\n",
            "(0.20806064728077037, 0.20806064728077037)\n",
            "epoch: 0 | TRAIN: loss 6017.84628 acc 5918.61463 | VALI: loss 1921.48432 acc 1921.48432\n",
            "epoch: 1 | TRAIN: loss 584.91304 acc 453.54285 | VALI: loss 9.37413 acc 9.37413\n",
            "epoch: 2 | TRAIN: loss 1.49788 acc 1.24512 | VALI: loss 0.7437 acc 0.7437\n",
            "epoch: 3 | TRAIN: loss 0.64881 acc 0.41774 | VALI: loss 0.68442 acc 0.68442\n",
            "epoch: 4 | TRAIN: loss 0.55999 acc 0.32996 | VALI: loss 0.60498 acc 0.60498\n",
            "epoch: 5 | TRAIN: loss 0.42001 acc 0.23304 | VALI: loss 0.51465 acc 0.51465\n",
            "epoch: 6 | TRAIN: loss 0.35212 acc 0.14845 | VALI: loss 0.4188 acc 0.4188\n",
            "epoch: 7 | TRAIN: loss 0.30875 acc 0.09118 | VALI: loss 0.3316 acc 0.3316\n",
            "epoch: 8 | TRAIN: loss 0.24481 acc 0.06086 | VALI: loss 0.24588 acc 0.24588\n",
            "epoch: 9 | TRAIN: loss 0.22876 acc 0.04825 | VALI: loss 0.18883 acc 0.18883\n",
            "Parameter containing:\n",
            "tensor([[[-2.0843,  2.0847]]], requires_grad=True)\n",
            "tensor(0.4827, grad_fn=<MeanBackward0>)\n",
            "(0.17008077525987755, 0.17008077525987755)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs44ze96kHZZ"
      },
      "source": [
        "<b>2G)</b> With the above choice of regularization parameter, what is the mean square error of the best network that you find on the test data? Try a few trials (5 or more) for each data test and report the value of the best network. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAN0k9wylOmz"
      },
      "source": [
        "\n",
        "#### We expect the training to be easier when there are fewer parameters to learn. Consider images of size $128\\times 1$ for the rest of the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnktFwXRkKNF"
      },
      "source": [
        "<b>2H)</b> Instead of resorting to regularization again, we may instead find a way to reduce the number of parameters. What additional layer can you add to the output of the convolution layer to reduce the number of parameters to be learned without losing any relevant information?\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2020_Fall/courseware/Week8/week8_homework/\">Refer to HW8 on MITx.</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXgOqKtRkNRP"
      },
      "source": [
        "<b>2I)</b> Add the layer you suggested above to your network and run some tests with data sets of size 1000 on $128\\times 1$ images.  How many parameters are left to learn with the new structure?\n",
        "\n",
        "You can find the appropriate documentations for the new types of modules mentioned in the previous problem here:\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/nn.html#dropout\">Dropout</a>\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/nn.html#maxpool1d\">MaxPool1d</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQGlJLxI__4A"
      },
      "source": [
        "# 3) MNIST (Digit Classification)\n",
        "\n",
        "In this section, we'll be looking at the MNIST data set seen already in problem 2. This time, we look at the *complete* MNIST problem where our networks will take an image of *any* digit from $0-9$ as input (recall that problem 2 only looked at digits $0$ and $1$) and try to predict that digit. Note that in general, an image is described as a two-dimensional array of pixels. Here, the image is a <a href=\"https://en.wikipedia.org/wiki/Grayscale\">grayscale</a> image, so each pixel is represented by only one integer value, in the range $0$ to $255$ (compared to RGB images where each pixel is represented by three integer values, encoding intensity levels in red, green, and blue color channels).\n",
        "\n",
        "Also, we will now use out-of-the-box neural network implementations using PyTorch. State-of-the-art systems have error rates of less than 0.5% percent on this data set (see <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\">this list</a>).  We'll be happy with an error rate less than 2% since we don't have all year...\n",
        "\n",
        "You can access the MNIST data for this problem using:\n",
        "<br><code>train, validation = get_MNIST_data()</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrIFeBFNWu38"
      },
      "source": [
        "def shifted(X, shift):\n",
        "    n = X.shape[0]\n",
        "    m = X.shape[1]\n",
        "    size = m + shift\n",
        "    X_sh = np.zeros((n, size, size))\n",
        "    plt.ion()\n",
        "    for i in range(n):\n",
        "        sh1 = np.random.randint(shift)\n",
        "        sh2 = np.random.randint(shift)\n",
        "        X_sh[i, sh1:sh1+m, sh2:sh2+m] = X[i, :, :]\n",
        "        # If you want to see the shifts, uncomment\n",
        "        #plt.figure(1); plt.imshow(X[i])\n",
        "        #plt.figure(2); plt.imshow(X_sh[i])\n",
        "        #plt.show()\n",
        "        #input('Go?')\n",
        "    return X_sh\n",
        "  \n",
        "def get_MNIST_data(shift=0):\n",
        "    train = MNIST(root='./mnist_data', train=True, download=True, transform=None)\n",
        "    val = MNIST(root='./mnist_data', train=False, download=True, transform=None)\n",
        "    (X_train, y1), (X_val, y2) = (train.data.numpy(), train.targets.numpy()), \\\n",
        "                                  (val.data.numpy(), val.targets.numpy())\n",
        "    if shift:\n",
        "        X_train = shifted(X_train, shift)\n",
        "        X_val = shifted(X_val, shift)\n",
        "    return (X_train, y1), (X_val, y2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZvlHZS3WjRA"
      },
      "source": [
        "You can run the fully connected MNIST model, using:\n",
        "<br><code>run_pytorch_fc_mnist(train, validation, layers, epochs)</code>\n",
        "\n",
        "And, you can run the CNN MNIST test, using:\n",
        "<br><code>run_pytorch_cnn_mnist(train, validation, layers, epochs)</code>\n",
        "\n",
        "You will need to design your own `layers` to feed to `run_pytorch_fc_mnist` and `run_pytorch_cnn_mnist`, which will be different than the ones specified by `archs()`. For instance, `layers=[nn.Linear(in_features=64, out_features=4)]` defines a single layer with 64 inputs and 4 output units.\n",
        "Note that the training procedure, uses <a href=\"https://pytorch.org/docs/stable/nn.html#crossentropyloss\">PyTorch's CrossEntropyLoss</a>, which handles the softmax activations for you, so adding a softmax layer to the end of your network is not necessary and will produce undesired results.\n",
        "Also, we advise you to use the option `verbose=True` when unsure about the progress made during training of your models.\n",
        "#### **IMPORTANT:** For this and subsequent questions, use the PyTorch implementation of modules. For example, for a linear layer, use <code>nn.Linear(...)</code>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx1jt6P9AUk1"
      },
      "source": [
        "<b> 3A)</b> Look at the code and indicate what the difference is between <code>run_pytorch_fc_mnist</code> and <code>run_pytorch_cnn_mnist</code>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5moSfb7CcXd"
      },
      "source": [
        "def make_deterministic():\n",
        "    torch.manual_seed(10)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(10)\n",
        "\n",
        "def weight_reset(l):\n",
        "    if isinstance(l, nn.Conv2d) or isinstance(l, nn.Linear):\n",
        "        l.reset_parameters()\n",
        "\n",
        "def run_pytorch_fc_mnist(train, test, layers, epochs, verbose=True, trials=1, deterministic=True):\n",
        "    '''\n",
        "    train, test = input data\n",
        "    layers = list of PyTorch layers, e.g. [nn.Linear(in_features=784, out_features=10)]\n",
        "    epochs = number of epochs to run the model for each training trial\n",
        "    trials = number of evaluation trials, resetting weights before each trial\n",
        "    '''\n",
        "    if deterministic:\n",
        "        make_deterministic()\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Flatten the images\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m * m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m * m))\n",
        "\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        for l in layers:\n",
        "            weight_reset(l)\n",
        "        # Make Dataset Iterables\n",
        "        train_iter, val_iter = make_iter(X_train, y1, batch_size=32), make_iter(X_val, y2, batch_size=32)\n",
        "        # Run the model\n",
        "        model, vacc, tacc = \\\n",
        "            run_pytorch(train_iter, val_iter, None, layers, epochs, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print(\"\\nAvg. validation accuracy:\" + str(val_acc / trials))\n",
        "    if test_acc:\n",
        "        print(\"\\nAvg. test accuracy:\" + str(test_acc / trials))\n",
        "\n",
        "\n",
        "def run_pytorch_cnn_mnist(train, test, layers, epochs, verbose=True, trials=1, deterministic=True):\n",
        "    if deterministic:\n",
        "        make_deterministic()\n",
        "    # Load the dataset\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Add a final dimension indicating the number of channels (only 1 here)\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], 1, m, m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], 1, m, m))\n",
        "\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        for l in layers:\n",
        "            weight_reset(l)\n",
        "        # Make Dataset Iterables\n",
        "        train_iter, val_iter = make_iter(X_train, y1, batch_size=32), make_iter(X_val, y2, batch_size=32)\n",
        "        # Run the model\n",
        "        model, vacc, tacc = \\\n",
        "            run_pytorch(train_iter, val_iter, None, layers, epochs, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print(\"\\nAvg. validation accuracy:\" + str(val_acc / trials))\n",
        "    if test_acc:\n",
        "        print(\"\\nAvg. test accuracy:\" + str(test_acc / trials))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sGfqAbICbmE"
      },
      "source": [
        "<b> 3B)</b> Using one epoch of training, what is the accuracy of a network **with no hidden units** (using the <code>run_pytorch_fc_mnist</code> method) on this data?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1VAxZ17DtPQ",
        "outputId": "a31ea833-1e11-405e-eceb-30d8cb5b7996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505,
          "referenced_widgets": [
            "d9efbac1fe5b458ebd179a2b4a9a9515",
            "0352462be63d4da9960ecc4be43b3a5c",
            "e897649130eb4fcf95b8e141edafd2bb",
            "41eddd7ce9c445d79608791e1727a5a2",
            "5de22933be6e4b349f0192228706ebf3",
            "e6128be59a1a48858f65152246dfb879",
            "d386dc2f6c9d421aa0dfb765c32c6e16",
            "15a388194c034446b05505e45a6e0a4d",
            "86fa5dfdb1b3454288c4d0e44787000e",
            "221d42c689b54e67b583cb162256da52",
            "e0aec35ed33e4f5f9258a4e961525d0c",
            "2b5939c97612423ead8222a7a6deced3",
            "f48d8ed7d86b4aeb83bf0f5b748bd222",
            "1ab8a43826264a6e965e8dace7be6b00",
            "5634b028a67d4395b5b9a0d765f9c9e8",
            "dea511217f8e49fb8f9958e03d3c1a7c",
            "50fdc515cac04020ad24e9ed1c89df0a",
            "61fdd9702b4e4b7ca6da0ad9dc4bdd31",
            "781b50aa9a5e470b934700a1dc52642f",
            "705d4967c1274453b0bc281c0edb287d",
            "8bdebe873b6a42979863a3c6827fca75",
            "7fb99ebaff5c42a1988b5f6d6c98af30",
            "b240b6a212aa4e0b8550c07baf491410",
            "fac615aeaaac4de9b1f8762cfd323d89",
            "b63cb234b99f47469fd41479e4df3903",
            "4379a90245fc410dac8b82751a899f72",
            "9feaf212a1004dc09ddcae2e619ec911",
            "49e3d6a35a734f268161c28a9e4c79e8",
            "f9ebf3365f7443ab8229ea8f30d19f0e",
            "106906d7790a4ffb93218d45f17ff2b7",
            "21e4a24d17724a77a99bc63d4e73b089",
            "04e126b5641e475bb0ea3c6b3aef5195"
          ]
        }
      },
      "source": [
        "train, validation = get_MNIST_data()\n",
        "run_pytorch_fc_mnist(train, validation, [nn.Linear(in_features=28*28, out_features=10)], 1, verbose=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9efbac1fe5b458ebd179a2b4a9a9515",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86fa5dfdb1b3454288c4d0e44787000e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50fdc515cac04020ad24e9ed1c89df0a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b63cb234b99f47469fd41479e4df3903",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "epoch: 0 | TRAIN: loss 0.18974 acc 0.8526 | VALI: loss 0.27195 acc 0.8418\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.8418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsQ31e1lDE6u"
      },
      "source": [
        "<b> 3C)</b> Now, linearly scale the input data so that the pixel values are between 0 and 1 and repeat your test with the original layer. What is the accuracy now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b-RcZu1EPaj",
        "outputId": "fb16416b-8659-4277-a4fe-11530b8ba8db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "layers = [nn.Linear(in_features=28*28, out_features=10)]\n",
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = ((train[0]- np.mean(train[0]))/np.std(train[0]), train[1])\n",
        "validation = ((validation[0]- np.mean(validation[0]))/np.std(validation[0]), validation[1])\n",
        "\n",
        "run_pytorch_fc_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.01177 acc 0.89287 | VALI: loss 0.01166 acc 0.8862\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.8862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrYGfcOLEr0f"
      },
      "source": [
        "### Important: <b>Always scale the data like in 3C) for subsequent problems.</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHoyqJdqDFH5"
      },
      "source": [
        "<b> 3E)</b> Using this same architecture, what is the accuracy after the 1st, 5th, 10th, and 15th epochs? Note that this colab notebook 0-indexes epoch output. We're looking for the first, fifth, tenth, and fifteenth number outputted by <code>run_pytorch_fc_mnist(train, validation, layers, 15, verbose=True)</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcd256Rosg-5"
      },
      "source": [
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = train[0] / 255, train[1]\n",
        "validation = validation[0] / 255, validation[1]\n",
        "#train = ((train[0]- np.mean(train[0]))/np.std(train[0]), train[1])\n",
        "#validation = ((validation[0]- np.mean(validation[0]))/np.std(validation[0]), validation[1])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8PlbWS_EwTv",
        "outputId": "81819bdf-340c-42e5-9f3f-0bb4a6a7f936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "layers = [nn.Linear(in_features=28*28, out_features=10)]\n",
        "run_pytorch_fc_mnist(train, validation, layers, 15, trials = 1, verbose=True)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.01463 acc 0.88193 | VALI: loss 0.00977 acc 0.9142\n",
            "epoch: 1 | TRAIN: loss 0.00962 acc 0.91462 | VALI: loss 0.00896 acc 0.9189\n",
            "epoch: 2 | TRAIN: loss 0.00898 acc 0.92017 | VALI: loss 0.00869 acc 0.9215\n",
            "epoch: 3 | TRAIN: loss 0.00865 acc 0.92353 | VALI: loss 0.00857 acc 0.9231\n",
            "epoch: 4 | TRAIN: loss 0.00845 acc 0.92527 | VALI: loss 0.00851 acc 0.923\n",
            "epoch: 5 | TRAIN: loss 0.0083 acc 0.92667 | VALI: loss 0.00847 acc 0.9232\n",
            "epoch: 6 | TRAIN: loss 0.00819 acc 0.9277 | VALI: loss 0.00846 acc 0.9231\n",
            "epoch: 7 | TRAIN: loss 0.0081 acc 0.92858 | VALI: loss 0.00845 acc 0.9234\n",
            "epoch: 8 | TRAIN: loss 0.00802 acc 0.92938 | VALI: loss 0.00845 acc 0.9237\n",
            "epoch: 9 | TRAIN: loss 0.00796 acc 0.9301 | VALI: loss 0.00845 acc 0.9237\n",
            "epoch: 10 | TRAIN: loss 0.0079 acc 0.9307 | VALI: loss 0.00846 acc 0.9236\n",
            "epoch: 11 | TRAIN: loss 0.00785 acc 0.93095 | VALI: loss 0.00847 acc 0.9238\n",
            "epoch: 12 | TRAIN: loss 0.00781 acc 0.93125 | VALI: loss 0.00849 acc 0.9242\n",
            "epoch: 13 | TRAIN: loss 0.00777 acc 0.9316 | VALI: loss 0.0085 acc 0.9243\n",
            "epoch: 14 | TRAIN: loss 0.00773 acc 0.93188 | VALI: loss 0.00852 acc 0.9241\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9225200000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPg23ukPstkZ"
      },
      "source": [
        "0.9142, 0.923, 0.9237, 0.9241"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCPTuz-tDFTg"
      },
      "source": [
        "<b> 3H)</b> Using one epoch of training, try a single hidden layer, followed by a ReLU activation layer before the final output layer, and gradually increase the units; specifically, try (128, 256, 512, 1024) units and observe the results.  What are the accuracies?\n",
        "To define a ReLU layer in pytorch simply use <code>ReLU()</code>.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmpbP_VHFoh1",
        "outputId": "5b566a41-33ec-4443-b948-945b3d74e43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "for num in [128,256,512,1024]:\n",
        "    layers = [nn.Linear(in_features=28*28, out_features=num), nn.ReLU(),nn.Linear(in_features=num, out_features=10)]\n",
        "    run_pytorch_fc_mnist(train, validation, layers, epochs=1, verbose=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.00938 acc 0.91697 | VALI: loss 0.00532 acc 0.9482\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9482\n",
            "epoch: 0 | TRAIN: loss 0.00823 acc 0.92465 | VALI: loss 0.00456 acc 0.9548\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9548\n",
            "epoch: 0 | TRAIN: loss 0.00724 acc 0.93263 | VALI: loss 0.00379 acc 0.9624\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9624\n",
            "epoch: 0 | TRAIN: loss 0.00651 acc 0.93792 | VALI: loss 0.00372 acc 0.9604\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEM5mZi5DFYS"
      },
      "source": [
        "<b> 3I)</b> Now, try a network with two hidden layers:\n",
        "<ul>\n",
        "  <li>A fully connected layer with 512 hidden units\n",
        "  <li> A ReLU activation layer\n",
        "  <li> A fully connected layer with 256 hidden units\n",
        "  <li> A ReLU activation layer\n",
        "  <li> A fully-connected layer with 10 output units\n",
        "    \n",
        "What is the accuracy?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cwp1VR7F06q",
        "outputId": "d3d2a95a-f8d7-4ab8-ee4b-f250aded8d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "layers = [nn.Linear(in_features=28*28, out_features=512), \n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=512, out_features=256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=256, out_features=10)]\n",
        "    \n",
        "run_pytorch_fc_mnist(train, validation, layers, 1, verbose=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.0068 acc 0.93513 | VALI: loss 0.004 acc 0.9593\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmnNUT2nDFdi"
      },
      "source": [
        "\n",
        "<b> 3J)</b> Build a convolutional network with the following structure:\n",
        "\n",
        "<ul>\n",
        "<li> A convolutional layer (without padding) with 32 filters of size 3 × 3\n",
        "<li> A ReLU activation layer\n",
        "<li> A max pooling layer with size 2 × 2\n",
        "<li> A convolutional layer (without padding) with 64 filters of size 3 × 3\n",
        "<li> A ReLU activation layer\n",
        "<li> A max pooling layer with size 2 × 2\n",
        "<li> A flatten layer (<b>will flatten to size 1600 = 5 × 5 × 64</b>; try to figure out why!)\n",
        "<li> A fully connected layer with 128 units \n",
        "<li> A ReLU activation layer\n",
        "<li> A dropout layer with drop probability 0.5\n",
        "<li> A fully-connected layer with 10 output units\n",
        "</ul>\n",
        "To define Convolutional and max pooling layers in PyTorch use the following syntax:\n",
        "<code> c = Conv2d(in_channels=i, out_channels=o, kernel_size=filter_size); m = MaxPool2d(kernel_size=filter_size) </code>, where <code> i </code> and <code> o</code> are integers and <code>filter_size</code> can either be an integer (for square filters) or a tuple (for non-square filters i.e. (2, 3) for 2x3 filter).\n",
        "Train it on MNIST for one epoch, using <code>run_pytorch_cnn_mnist</code> (this may take a little while).  What is the accuracy on the validation set?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-sPW8xKF7EZ",
        "outputId": "eee52cea-22db-489f-b9ff-412a6d94d2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "c_1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "c_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "m = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "layers = [c_1, \n",
        "          nn.ReLU(),\n",
        "          m,\n",
        "          c_2,\n",
        "          nn.ReLU(),\n",
        "          m,\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(in_features=1600, out_features=128), \n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=0.5),\n",
        "          nn.Linear(in_features=128, out_features=10)\n",
        "          ]\n",
        "\n",
        "run_pytorch_cnn_mnist(train, validation, layers, epochs = 1, verbose=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.00778 acc 0.92457 | VALI: loss 0.0019 acc 0.9813\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjmqgGvIDFiS"
      },
      "source": [
        "<b> 3K)</b> Now, let's compare the performance of a fully connected model and a CNN on data where the characters have been shifted randomly so that they are no longer centered.  \n",
        "\n",
        "You can build such a data set by calling: <code>train_20, validation_20 = get_MNIST_data(shift=20)</code>. Remember to scale it appropriately.\n",
        "\n",
        "<b>Note that each image is now 48x48, so you will need to change your layer definitions (size after Flatten will be 6400)</b>.\n",
        "Run your two-hidden-layer FC architecture from above (problem 3I) on this data and then run the CNN architecture from above (problem 3J), both for one epoch. Report your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvfiyrN9Gf7X",
        "outputId": "ea653b46-cf8b-451f-c56d-ada077de01e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "train_20, validation_20 = get_MNIST_data(shift=20)\n",
        "\n",
        "# Scale the images\n",
        "train_20 = (train_20[0]/255, train_20[1]) \n",
        "validation_20 = (validation_20[0]/255, validation_20[1])  \n",
        "\n",
        "\n",
        "layers_fc = [nn.Linear(in_features=48*48, out_features=512), \n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=512, out_features=256),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(in_features=256, out_features=10)]\n",
        "\n",
        "run_pytorch_fc_mnist(train_20, validation_20, layers_fc, 1, verbose=True)\n",
        "\n",
        "c_1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "c_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "m = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "layers_cnn = [c_1, \n",
        "          nn.ReLU(),\n",
        "          m,\n",
        "          c_2,\n",
        "          nn.ReLU(),\n",
        "          m,\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(in_features=6400, out_features=128), \n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(p=0.5),\n",
        "          nn.Linear(in_features=128, out_features=10)\n",
        "          ]\n",
        "          \n",
        "run_pytorch_cnn_mnist(train_20, validation_20, layers_cnn, 1, verbose=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 | TRAIN: loss 0.02352 acc 0.74727 | VALI: loss 0.01352 acc 0.8629\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.8629\n",
            "epoch: 0 | TRAIN: loss 0.02721 acc 0.70413 | VALI: loss 0.00841 acc 0.9259\n",
            "\n",
            "\n",
            "Avg. validation accuracy:0.9259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4ZfEWIBB4B"
      },
      "source": [
        "# 4) Raining Cats and Dogs\n",
        "\n",
        "In this problem, we are going to explore how a model trained on a particular dataset behaves in the general population. We will use the following functions (and [generator](https://wiki.python.org/moin/Generators))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7JiISTWFi0t"
      },
      "source": [
        "def load_images(directory, imdir='./'):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for i in os.listdir(os.path.join(imdir, directory)):\n",
        "        img = resize(imread(os.path.join(imdir, directory, i)), (224, 224), anti_aliasing=True)\n",
        "        imgs.append(np.moveaxis(img, 2, 0))\n",
        "        if 'cat' in i:\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(1)\n",
        "    imgs = np.array(imgs)\n",
        "    return imgs, labels\n",
        "\n",
        "def data_gen(train_images, train_labels, batch_size, eval=True):\n",
        "    all_idxs = np.arange(len(train_labels))\n",
        "    idxs = np.random.shuffle(all_idxs)\n",
        "    i = 0\n",
        "    while i * batch_size + batch_size < train_images.shape[0]:\n",
        "        samples = train_images[i*batch_size: (i+1)*batch_size]\n",
        "        sample_labels = train_labels[i*batch_size: (i+1)*batch_size]\n",
        "        i += 1\n",
        "        \n",
        "        yield torch.tensor(samples, dtype=torch.float), torch.tensor(sample_labels, dtype=torch.long)\n",
        "    if eval:\n",
        "        samples = train_images[(i)*batch_size:]\n",
        "        sample_labels = train_labels[(i)*batch_size:]\n",
        "        yield torch.tensor(samples, dtype=torch.float), torch.tensor(sample_labels, dtype=torch.long)\n",
        "def postproc_output(out):\n",
        "  sm = torch.nn.Softmax(dim=1)\n",
        "  return sm(out).detach().numpy()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-07d6KNTFjB_"
      },
      "source": [
        "**4A)** Write code to evaluate the model on each of the three test sets, following this pseudocode:\n",
        "<ol>\n",
        "    <li> Load model \n",
        "    <ul>\n",
        "      <li> <tt> squeezenet_trained_cats_v_dogs.pt</tt> contains the <tt>state_dict</tt> of the model\n",
        "        <li> You will need to instantiate the model architecture first by running:\n",
        "          <tt>model = torchvision.models.squeezenet1_1(num_classes=2)</tt>\n",
        "        <li> Then use <a href=\"https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict\"><tt> model.load_state_dict</tt> </a>; make sure to use the parameter <tt>map_location=torch.device('cpu')</tt> when you use <tt>torch.load</tt>. It might also be helpful to read about the general workflow of <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference\"> saving and loading trained models in Pytorch </a>\n",
        "      <li> Make sure that you are in evaluation mode by using <tt> model.eval() </tt>\n",
        "    </ul>\n",
        "  <li> Load data from <tt>data_path</tt> [Use our function <tt>load_images</tt>]\n",
        "  <li> For each batch of data [Use our generator <tt>data_gen</tt>; note that the batch size doesn't really matter here except to keep from having to multiply matrices that are too large. For ease of implementation we suggest <tt>batch_size=1</tt>]:\n",
        "    <ul>\n",
        "      <li> Pass the batch of <tt>data</tt> through the model using <tt>model(data)</tt>\n",
        "        <li> Convert the predictions of the model to guesses (after softmax) [use our function <tt>postproc_output</tt>]\n",
        "      <li> Compare the guesses to the actual <tt>labels</tt>\n",
        "    </ul>\n",
        "    <li> Total the accuracy\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIG9E0NgBjo6"
      },
      "source": [
        "def evaluate_model(model_path, data_path):\n",
        "    # Load model \n",
        "    model = torchvision.models.squeezenet1_1(num_classes=2)                           # Instantiate model architecture\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))   # Load trained model\n",
        "    model.eval()                                                                      # Ensure that you are in evaluation mode\n",
        "\n",
        "    # Load data\n",
        "    batch_size = 1\n",
        "    imgs, labels = load_images(data_path) # Your code here (call load_images on data_path)\n",
        "    data_generator =  data_gen(imgs, labels, batch_size=1) # Your code here\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    # Iterate through data, label in data generator\n",
        "    for i, (data, lab) in enumerate(data_generator):\n",
        "        total += 1\n",
        "        predict = np.argmax(postproc_output(model(data))[0])\n",
        "        true_lab = lab.item()\n",
        "        if predict == true_lab:\n",
        "            correct += 1\n",
        "    print(correct/total)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "111yzeXLB2lH"
      },
      "source": [
        "Calculate the performance on each of the test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtF_dnjXid6f",
        "outputId": "117bd3a9-5def-4ddf-f8d2-d6917f491809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model_path = 'squeezenet_trained_cats_v_dogs.pt'\n",
        "for data_path in ['test1', 'test2', 'test3']:\n",
        "    evaluate_model(model_path, data_path)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.98\n",
            "0.49\n",
            "0.055\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}